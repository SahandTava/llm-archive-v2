# LLM Archive V2

A fast, focused tool for importing and searching LLM conversation archives.

## ðŸš€ Key Features

- **Sub-second search** across millions of messages using SQLite FTS5
- **50ms cold start** with single binary deployment
- **Import support** for ChatGPT, Claude, Gemini, XAI, and more
- **Desktop-first UI** with keyboard navigation
- **< 20MB memory usage** even with large archives

## ðŸ“Š Performance Targets

| Metric | Target | V1 Baseline |
|--------|--------|-------------|
| Cold Start | < 50ms | 3-5 seconds |
| Search Response | < 100ms | Broken |
| Memory Usage | < 20MB | 300MB+ |
| Import Speed | 10k msgs/sec | ~100 msgs/sec |
| Binary Size | < 15MB | N/A (Python) |

## ðŸ—ï¸ Architecture

```
Single Rust Binary
    â”œâ”€â”€ Web Server (Axum)
    â”œâ”€â”€ SQLite + FTS5
    â”œâ”€â”€ PyO3 Bridge (temporary)
    â””â”€â”€ Embedded Assets
```

## ðŸš¦ Quick Start

```bash
# Install and run
cargo build --release
./target/release/llm-archive

# Import conversations
llm-archive import chatgpt ./exports/conversations.json
llm-archive import claude ./exports/claude-*.json

# Start web server
llm-archive serve --port 8080
```

## ðŸ“ Project Structure

```
src/
â”œâ”€â”€ main.rs              # Entry point
â”œâ”€â”€ server/              # Web server (Axum)
â”œâ”€â”€ models/              # Domain models
â”œâ”€â”€ db/                  # Database layer
â”œâ”€â”€ import/              # Import pipelines
â”‚   â”œâ”€â”€ mod.rs          # Import orchestration
â”‚   â”œâ”€â”€ python_bridge.rs # PyO3 integration
â”‚   â””â”€â”€ parsers/        # Native Rust parsers
â”œâ”€â”€ search/              # FTS5 search
â””â”€â”€ templates/           # Askama templates
```

## ðŸ”„ Migration Status

- [x] Core architecture setup
- [x] Database schema (simplified to 4 tables)
- [ ] PyO3 bridge for Python parsers
- [ ] Native ChatGPT parser
- [ ] Native Claude parser
- [ ] Search implementation
- [ ] Web UI
- [ ] Keyboard navigation

## ðŸ§ª Testing

```bash
# Run all tests
cargo test

# Run with golden test data
cargo test --features golden-tests

# Benchmark performance
cargo bench
```

## ðŸ“ˆ Observability

Built-in metrics exposed at `/metrics`:
- Request latency histograms
- Search performance metrics
- Import throughput counters
- Memory usage gauges

## ðŸ”§ Configuration

```toml
# config.toml
[database]
path = "./llm_archive.db"
wal_mode = true
mmap_size = 1073741824  # 1GB

[search]
max_results = 100
snippet_length = 200

[import]
batch_size = 1000
python_bridge = true  # Use Python parsers initially
```

## ðŸ¤ Contributing

1. Keep it simple - no unnecessary abstractions
2. Performance first - profile before optimizing
3. Test with real data - use golden test files
4. Document decisions - update ARCHITECTURE.md

## ðŸ“ License

MIT License - see LICENSE file for details